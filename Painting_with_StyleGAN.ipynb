{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmoso13/Painting_with_StyleGAN/blob/main/Painting_with_StyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Painting with StyleGAN ðŸŽ¨\n",
        "### Hello ðŸ‘‹ welcome to the painting with stylegan notebook. This notebook is intended to \n",
        "1. Help explore stylegan's latent space using variational autoencoders as a sort of filter and\n",
        "2. Produce animations interpolating through stylegan's latent space using VAE \"palettes\" as a way to more graphically interface with the complicated space\n",
        "\n",
        "The code below will train a VAE to represent portions of SG's 512-D latent space in 2-D. This representation will be plotted on a regular x/y axis for you to explore, and provides the optional injection of 'color' to your palette to explore less likely sections of the space. Once happy with a palette, get to painting! Draw a line through the space to interp from one image to another, draw a curve to pass through a third section, or draw a circle to create an interp that starts in one location and circles away and around back to the beginning to create loops. Hope you have fun painting!\n",
        "-[@jmoso13](https://www.twitter.com/jmoso13)\n",
        "\n",
        "Tutorial Video: https://youtu.be/pkYHMPoZrkg"
      ],
      "metadata": {
        "id": "DF5E2yqpSby3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeLRKqX9-OXR"
      },
      "source": [
        "# Download Things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y9peDvAJoYy"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oUly-q_qEedc"
      },
      "outputs": [],
      "source": [
        "#@title Choose StyleGAN and Tensor Package Versions\n",
        "import re\n",
        "import os\n",
        "\n",
        "supported_versions = 'stylegan3 | pytorch'#@param ['stylegan2 | tensorflow', 'stylegan3 | pytorch']\n",
        "stylegan_version = int(re.findall('[0-9]+', supported_versions)[0]) \n",
        "tensor_pkg_version = supported_versions.split(' | ')[-1]\n",
        "\n",
        "print(f'using\\nstylegan version: {stylegan_version}\\ntensor package version: {tensor_pkg_version}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vOsgZJW5GMmk"
      },
      "outputs": [],
      "source": [
        "#@title Ensure Version Compatibility\n",
        "if os.getcwd() == '/content':\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    %tensorflow_version 1.x\n",
        "    !pip install h5py==2.10.0\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    !pip install Ninja opensimplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "avg_lG91u8cW"
      },
      "outputs": [],
      "source": [
        "#@title Clone Repos\n",
        "%%capture\n",
        "if os.getcwd() == '/content':\n",
        "  !git clone https://github.com/jmoso13/mod-drawing-tool.git\n",
        "  !git clone https://github.com/jmoso13/StyleGAN2-Latent-Tool.git\n",
        "  if stylegan_version == 2:\n",
        "    !git clone https://github.com/pbaylies/stylegan2-ada.git\n",
        "    %cd /content/stylegan2-ada/\n",
        "  elif stylegan_version == 3:\n",
        "    !git clone https://github.com/dvschultz/stylegan3\n",
        "    %cd stylegan3\n",
        "\n",
        "\n",
        "  !pip install -e /content/StyleGAN2-Latent-Tool/\n",
        "  %mv /content/StyleGAN2-Latent-Tool/latent_utils .\n",
        "  %mv /content/StyleGAN2-Latent-Tool/js_latent_utils.egg-info .\n",
        "  %mv /content/mod-drawing-tool/mods ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1NNwHo-KY4"
      },
      "source": [
        "# Import Things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wYVJ22eu-Sk"
      },
      "outputs": [],
      "source": [
        "if tensor_pkg_version == 'tensorflow':\n",
        "  import tensorflow as tf\n",
        "  print(tf.__version__)\n",
        "  from latent_utils.latent_utils import get_vae_dict, build_decoder, make_decoder_input, decode_pic, get_model_callbacks\n",
        "  from latent_utils.gan_utils import convertZtoW\n",
        "elif tensor_pkg_version == 'pytorch':\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F\n",
        "  import torch.optim as optim\n",
        "  from torch.utils.data import DataLoader, TensorDataset\n",
        "  import legacy\n",
        "  from latent_utils.latent_utils_torch import VAE\n",
        "import dnnlib\n",
        "import requests\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from mods.paths import CirclePath, LinePath\n",
        "from mods.modulated_path import ModulatedPath\n",
        "from mods import receivers, translators, voices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VseXi0F-Vyk"
      },
      "source": [
        "# Load Model\n",
        "Here is where you can specify location of a stylegan2 pretrained pkl from file or from url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8tdGPdhv-Vv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Point to location of network-snapshot pkl\n",
        "use_drive = True #@param{type: 'boolean'}\n",
        "#@markdown leave this ^^ checked to connect to google drive\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  root_path = '/content/drive/MyDrive/AI/StyleGan_Drawing'\n",
        "else:\n",
        "  root_path = os.getcwd()\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "model_path = f'{root_path}/models'\n",
        "createPath(model_path)\n",
        "palette_path = f'{model_path}/vae_palettes'\n",
        "createPath(palette_path)\n",
        "network_pkl_path = f'{model_path}/network_pkls'\n",
        "createPath(network_pkl_path)\n",
        "sketches_path = f'{root_path}/sketches_out'\n",
        "createPath(sketches_path)\n",
        "reel_path = f'{root_path}/reel'\n",
        "createPath(reel_path)\n",
        "\n",
        "network_pkl = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-metfaces-1024x1024.pkl\"#@param{type: 'string'}\n",
        "\n",
        "if stylegan_version == 2:\n",
        "  # Start TF session and load network_pkl\n",
        "  dnnlib.tflib.init_tf()\n",
        "  with dnnlib.util.open_url(network_pkl) as fp:\n",
        "    _G, _D, Gs = pkl.load(fp)\n",
        "  # Some setup\n",
        "  noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "  Gs_kwargs = dnnlib.EasyDict()\n",
        "  Gs_kwargs.randomize_noise = False\n",
        "  Gs_kwargs.output_transform = dict(func=dnnlib.tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "elif stylegan_version == 3:\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f'torch_device: {device}')\n",
        "  with dnnlib.util.open_url(network_pkl) as f:\n",
        "    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck0CPFa4_By2"
      },
      "source": [
        "# Create VAE Palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cGsSQGUv-CzK"
      },
      "outputs": [],
      "source": [
        "#@title Build Train/Test Data for our VAE, \n",
        "#@markdown sampled from normal Z space (mean==0, std==1)\n",
        "#@markdown then converted to GAN disentangled W space\n",
        "truncation_psi = 0.7#@param{type:'number'}\n",
        "truncation_cutoff = False#@param{type:'boolean'}\n",
        "if truncation_cutoff:\n",
        "  truncation_cutoff_val = 100#@param{type:'number'}\n",
        "else:\n",
        "  truncation_cutoff_val = None\n",
        "print(truncation_cutoff_val)\n",
        "#@markdown these parameters have to do with bounding the GAN space, leave at defaults or try lowering/raising the parameters slightly. lower values should provide more coherency and less variety, while higher values should increase variety at the cost of coherency.\n",
        "\n",
        "# Build val data set from 15000 random samples of Z space \n",
        "val_data_z = np.random.randn(15000, 512)\n",
        "val_data = list()\n",
        "i = 0\n",
        "\n",
        "# Convert to W space\n",
        "if tensor_pkg_version == 'tensorflow':\n",
        "  print('building val set...')\n",
        "  for vi in val_data_z:\n",
        "    if i % 5000 == 0:\n",
        "      print(i, ' datapoints built')\n",
        "    latent = vi.reshape(1,-1)\n",
        "    val_data.append(convertZtoW(Gs, latent)[:, 0][0])\n",
        "    i += 1\n",
        "  val_data = np.array(val_data)\n",
        "elif tensor_pkg_version == 'pytorch':\n",
        "  zs = torch.from_numpy(val_data_z).to(device)\n",
        "  ws = G.mapping(z=zs, c=None, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff_val)\n",
        "  _ = G.synthesis(ws[:1]) # warm up\n",
        "  val_data = ws[:, 0, :].detach().cpu().numpy()\n",
        "\n",
        "# Build train dataset from 100000 random samples of Z space\n",
        "train_data_z = np.random.randn(100000, 512)\n",
        "train_data = list()\n",
        "i = 0\n",
        "\n",
        "# Convert to W space\n",
        "if tensor_pkg_version == 'tensorflow':\n",
        "  print('building train set...')\n",
        "  for ti in train_data_z:\n",
        "    if i % 5000 == 0:\n",
        "      print(i, ' datapoints built')\n",
        "    latent = ti.reshape(1,-1)\n",
        "    train_data.append(convertZtoW(Gs, latent)[:, 0][0])\n",
        "    i += 1\n",
        "  train_data = np.array(train_data)\n",
        "elif tensor_pkg_version == 'pytorch':\n",
        "  zs = torch.from_numpy(train_data_z).to(device)\n",
        "  ws = G.mapping(z=zs, c=None, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff_val)\n",
        "  _ = G.synthesis(ws[:1]) # warm up\n",
        "  train_data = ws[:, 0, :].detach().cpu().numpy()\n",
        "\n",
        "# Inspect\n",
        "print(f\"Train Size: {train_data.shape}\\nVal Size: {val_data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rdr2UIsA_pS9"
      },
      "outputs": [],
      "source": [
        "#@title Setup for VAE\n",
        "model_name = 'styleGAN_VAE' #@param{type: 'string'}\n",
        "nn_vae = False#@param{type:'boolean'}\n",
        "#@markdown nn_vae is a variant on a vae only usable with tensorflow\n",
        "\n",
        "if tensor_pkg_version == 'tensorflow':\n",
        "  hidden_layers = 8\n",
        "  lowest_dim = 2\n",
        "\n",
        "  vae_dict = get_vae_dict(512, hidden_layers, lowest_dim, model_name, batch_norm=True, nn_vae=nn_vae)\n",
        "  vae = vae_dict['vae']\n",
        "  encoder = vae_dict['encoder']\n",
        "\n",
        "elif tensor_pkg_version == 'pytorch':\n",
        "  vae = VAE().to(device)\n",
        "  optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "  train_tensor = torch.Tensor(train_data).to(device)\n",
        "  train_dataset = TensorDataset(train_tensor)\n",
        "  val_tensor = torch.Tensor(val_data).to(device)\n",
        "  print(vae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9n4nFlxp_zxT"
      },
      "outputs": [],
      "source": [
        "#@title Train VAE Palette\n",
        "mode = 'train'#@param['train', 'load']\n",
        "load_palette_path = ''#@param{type:'string'}\n",
        "#@markdown **load_palette_path** - if mode == load, load palette from this path instead of training. this parameter is ignored if mode == train\n",
        "\n",
        "#@markdown Hyperparameters (shouldn't need to tweak unless interested)\n",
        "batch_size = 256#@param{type: 'number'}\n",
        "epochs = 3000#@param{type: 'number'}\n",
        "kld_weight = 0.6#@param{type: 'number'}\n",
        "recons_weight = 0.4#@param{type: 'number'}\n",
        "patience = 15#@param{type: 'number'}\n",
        "#@markdown **batch size** - how many training examples per batch, more means faster epochs but slower convergence, should be a power of 2\n",
        "\n",
        "#@markdown **epochs** - how many times the training will loop through the entire dataset, interacts with patience which stops training early when the algorithm stops learning\n",
        "\n",
        "#@markdown **kld_weight** - controls how much the latent space will be normally distributed (should be a value between 0-1, a weight of 0 is equivalent to a vanilla autoencoder)\n",
        "\n",
        "#@markdown **patience** - controls how many epochs VAE will continue to train for without improvement before ending training \n",
        "\n",
        "if mode == 'train':\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    # Train\n",
        "    es,mc = get_model_callbacks(model_name, patience=patience, model_dir_path=palette_path) # Patience determines how many epochs model will allow training to continue without improvement in val dataset\n",
        "    vae.fit(train_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            validation_data=(val_data ,None),\n",
        "            callbacks=[es, mc])\n",
        "\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    p_count = 0\n",
        "    smallest_v_loss = 1000000000\n",
        "\n",
        "    for epoch in range(epochs):    \n",
        "      running_loss = 0.0\n",
        "      vae.train()\n",
        "      for i, data in enumerate(dataloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data[0]\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs, mu, log_var = vae(inputs)\n",
        "        recons_loss = F.mse_loss(outputs, inputs)*512\n",
        "        kld_loss = -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = -1)\n",
        "        loss = torch.mean(recons_weight*recons_loss + kld_weight*kld_loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 40 == 39:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 40:.3f}, kld_loss: {torch.mean(kld_loss)}, recons_loss: {recons_loss/512}')\n",
        "            running_loss = 0.0\n",
        "      vae.eval()\n",
        "      p_count += 1\n",
        "      with torch.no_grad():\n",
        "        v_outputs, v_mu, v_log_var = vae(val_tensor)\n",
        "        v_recons_loss = F.mse_loss(v_outputs, val_tensor)*512\n",
        "        v_kld_loss = -0.5 * torch.sum(1 + v_log_var - v_mu ** 2 - v_log_var.exp(), dim = -1)\n",
        "        v_loss = torch.mean(recons_weight*v_recons_loss + kld_weight*v_kld_loss)\n",
        "        if v_loss < smallest_v_loss:\n",
        "          smallest_v_loss = v_loss\n",
        "          p_count = 0\n",
        "          torch.save(vae.state_dict(), f'{palette_path}/best_{model_name}.pt')\n",
        "        print(f'\\n\\n EPOCH: {epoch + 1}  |  VAL LOSS: {v_loss}  |  PATIENCE COUNT: {p_count}\\n')\n",
        "      if p_count > patience:\n",
        "        print(f'Patience reached at {epoch} epochs')\n",
        "        break\n",
        "\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    vae.load_weights(f'{palette_path}/best_{model_name}.h5')\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    vae.load_state_dict(torch.load(f'{palette_path}/best_{model_name}.pt'))\n",
        "    vae.eval() \n",
        "elif mode == 'load':\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    vae.load_weights(load_palette_path)\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    vae.load_state_dict(torch.load(load_palette_path))\n",
        "    vae.eval() \n",
        "else:\n",
        "  print('mode must be either train or load')\n",
        "\n",
        "inject_image = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maD0DVOlJQBp"
      },
      "source": [
        "# Inject Image - (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fdjWfsEzxUR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Inject Image - (Optional)\n",
        "noise = 0.0\n",
        "inject_image = False#@param{type:'boolean'}\n",
        "#@markdown **inject_image** - color your latent space palette with a randomly sampled image. re-run this cell until you get an image you like then run the next cell to display the latent space palette.\n",
        "image_weight = 0.6#@param{type:'number'}\n",
        "#@markdown **image_weight** - how far you want to skew your palette towards the injected image. a value of 0 makes no change, a value of 1 should guarantee the injected image shows up in the palette. values >1 will skew your palette past the image into brand new exciting territories.\n",
        "\n",
        "if inject_image:\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    num_w = Gs.components.synthesis.input_shape[1]\n",
        "    rint = np.random.randint(100000)\n",
        "    noise = train_data[[rint]]\n",
        "    noise = np.repeat(noise, num_w, axis=0).reshape(1, num_w, -1)\n",
        "    img = Gs.components.synthesis.run(noise, **Gs_kwargs)[0]\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    num_w = G.synthesis.num_ws\n",
        "    rint = np.random.randint(100000)\n",
        "    noise = train_data[[rint]]\n",
        "    noise = np.repeat(noise, num_w, axis=0).reshape(1, num_w, -1)\n",
        "    img = G.synthesis(ws = torch.tensor(noise).to(device), noise_mode='const')[0]\n",
        "    img = (img.permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).detach().cpu().numpy()\n",
        "\n",
        "  im = Image.fromarray(img.astype('uint8')).resize((256, 256), Image.LANCZOS)\n",
        "  display(im)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDf6SJZsriUk"
      },
      "source": [
        "# Display Palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-c4tdSESHBay"
      },
      "outputs": [],
      "source": [
        "#@title Display Latent Space Palette\n",
        "dim =  2#@param{type: 'number'}\n",
        "#@markdown **dim** - controls how zoomed out you are on the VAE latent space. zoom in and out, see what you find! (higher -> out, lower -> in)\n",
        "num =  10#@param{type: 'number'}\n",
        "#@markdown **num** - controls how many images are displayed per side of your grid. more will show finer differences between each picture. too many can cause the notebook to run out of RAM and crash.\n",
        "\n",
        "if tensor_pkg_version == 'tensorflow':\n",
        "  decoder_input = make_decoder_input(2)\n",
        "  decoder = build_decoder(vae, decoder_input)\n",
        "  pic_size = Gs.output_shape[-1]\n",
        "  num_w = Gs.components.synthesis.input_shape[1]\n",
        "  points = np.linspace(-dim, dim, num)\n",
        "  full_pic = np.zeros((pic_size*len(points), pic_size*len(points), 3))\n",
        "  for i, y in enumerate(points):\n",
        "    for j, x in enumerate(points):\n",
        "      # print(x, y)\n",
        "      if inject_image:\n",
        "        img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs, noise=noise, noise_weight=image_weight)\n",
        "      else:\n",
        "        img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs)\n",
        "      full_pic[pic_size*len(points)-(i*pic_size+pic_size):pic_size*len(points)-i*pic_size, j*pic_size:j*pic_size+pic_size] = img\n",
        "elif tensor_pkg_version == 'pytorch':\n",
        "  vae.eval()\n",
        "  pic_size = G.synthesis(ws[:1]).shape[-1]\n",
        "  num_w = G.synthesis.num_ws\n",
        "  points = np.linspace(-dim, dim, num)\n",
        "  full_pic = np.zeros((pic_size*len(points), pic_size*len(points), 3))\n",
        "  for i, y in enumerate(points):\n",
        "    for j, x in enumerate(points):\n",
        "      # print(x, y)\n",
        "      zij = vae.decode(torch.tensor(np.array([[x,y]]).astype('float32')).to(device))\n",
        "      zij = np.repeat(zij.detach().cpu().numpy(), num_w, axis=0).reshape(1, num_w, -1)\n",
        "      if inject_image:\n",
        "        zij += image_weight*noise\n",
        "      img = G.synthesis(ws = torch.tensor(zij).to(device), noise_mode='const')[0]\n",
        "      img = (img.permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).detach().cpu().numpy()\n",
        "      full_pic[pic_size*len(points)-(i*pic_size+pic_size):pic_size*len(points)-i*pic_size, j*pic_size:j*pic_size+pic_size] = img\n",
        "      \n",
        "im = Image.fromarray(full_pic.astype('uint8')).resize((2048, 2048), Image.LANCZOS)\n",
        "\n",
        "display_size = 'medium'#@param['small', 'medium', 'large', 'xlarge']\n",
        "fgs_dict = dict(small=10, medium=16, large=26, xlarge=40)\n",
        "fgs = fgs_dict[display_size]\n",
        "fig, ax = plt.subplots(figsize=(fgs,fgs))\n",
        "ax.imshow(im, extent=[-dim,dim,-dim,dim])\n",
        "label_list = [round(p, 3) for p in points]\n",
        "mn = points.min()\n",
        "mx = points.max()\n",
        "sz = mx-mn\n",
        "diff = sz/(len(points)-1)\n",
        "ax_points = np.linspace(mn+diff/2, (mx-diff)+diff/2, num)\n",
        "ax.set_xticks(ax_points)\n",
        "ax.set_xticklabels(label_list)\n",
        "ax.set_yticks(ax_points)\n",
        "ax.set_yticklabels(label_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vBSGuw3vrqQS"
      },
      "outputs": [],
      "source": [
        "#@title Save Palette\n",
        "save_palette = True#@param{type:'boolean'}\n",
        "save_name = 'palette_1'#@param{type:'string'}\n",
        "\n",
        "if save_palette:\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    vae.save_weights(f'{palette_path}/{model_name}_{save_name}.h5')\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    torch.save(vae.state_dict(), f'{palette_path}/{model_name}_{save_name}.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhU4dvSmrwcV"
      },
      "source": [
        "# Paint with VAE Palette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wxKQZkmErvMF"
      },
      "outputs": [],
      "source": [
        "#@title Start and End Points\n",
        "#@markdown Pick a start point and end point to draw through the 2D palette. Can choose line, curve, or circle path. Line path will connect the points in a straight line, curve can be a bit unexpected but will add a curving path, circle path will create a video that loops back to where it started. For circles, the start_point_x and start_point_y coordinates are the center of the circle and end_points are ignored. Path will be plotted so you can inspect what video you are getting. Videos are saved in a folder called sketches_out.\n",
        "start_point_x = 0#@param{type:'number'}\n",
        "start_point_y = 0#@param{type:'number'}\n",
        "\n",
        "#@markdown ---------------------\n",
        "\n",
        "end_point_x = None#@param{type:'number'}\n",
        "end_point_y = None#@param{type:'number'}\n",
        "\n",
        "#@markdown ---------------------\n",
        "\n",
        "fps = 59.94#@param{type:'number'}\n",
        "seconds = 8#@param{type:'number'}\n",
        "path = 'line'#@param['line', 'curve', 'circle']\n",
        "curve_intensity = 2.0#@param{type:'number'}\n",
        "frames = int(fps*seconds)\n",
        "flip_curve = False#@param{type:'boolean'}\n",
        "circle_radius = 1#@param{type:'number'}\n",
        "\n",
        "if path == 'line':\n",
        "  l = LinePath((start_point_x, start_point_y), (end_point_x, end_point_y), frames)\n",
        "  iter(l)\n",
        "elif path == 'curve':\n",
        "  ll = LinePath((start_point_x, start_point_y), (end_point_x, end_point_y), frames)\n",
        "  c = CirclePath(frames=frames, rotations=0.5, center=(start_point_x, start_point_y), radius=curve_intensity)\n",
        "  va = receivers.vec_avg\n",
        "  if not flip_curve:\n",
        "    cd = translators.PathTranslator(c, 'direc')\n",
        "  else:\n",
        "    cd = translators.CMultiplyTranslator(translators.PathTranslator(c, 'direc'), -1.0)\n",
        "  mod = [(cd, va, 'direc')]\n",
        "  l = ModulatedPath(ll, *mod)\n",
        "  iter(l)\n",
        "elif path == 'circle':\n",
        "  l = CirclePath(frames=frames, rotations=1, center=(start_point_x, start_point_y), radius=circle_radius)\n",
        "  iter(l)\n",
        "\n",
        "xs = list()\n",
        "ys = list()\n",
        "for tick in range(frames):\n",
        "  x,y = next(l)\n",
        "  xs.append(x)\n",
        "  ys.append(y)\n",
        "\n",
        "m = 'Start'\n",
        "for x, y in [(xs[0], ys[0]), (xs[-1], ys[-1])]:\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    if inject_image:\n",
        "      img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs, noise=noise, noise_weight=image_weight)\n",
        "    else:\n",
        "      img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs)\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    vae.eval()\n",
        "    zij = vae.decode(torch.tensor(np.array([[x,y]]).astype('float32')).to(device))\n",
        "    zij = np.repeat(zij.detach().cpu().numpy(), num_w, axis=0).reshape(1, num_w, -1)\n",
        "    if inject_image:\n",
        "      zij += image_weight*noise\n",
        "    img = G.synthesis(ws = torch.tensor(zij).to(device), noise_mode='const')[0]\n",
        "    img = (img.permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).detach().cpu().numpy()\n",
        "  print(f'\\n\\n{m} Pic:\\n\\n')\n",
        "  display(Image.fromarray(img.astype('uint8'), 'RGB').resize((256,256), Image.LANCZOS))\n",
        "  m = 'End'\n",
        "\n",
        "print('\\n\\nPath Preview:\\n\\n')\n",
        "fgs = fgs_dict[display_size]\n",
        "fig, ax = plt.subplots(figsize=(fgs,fgs))\n",
        "ax.imshow(im, extent=[-dim,dim,-dim,dim])\n",
        "label_list = [round(p, 3) for p in points]\n",
        "mn = points.min()\n",
        "mx = points.max()\n",
        "sz = mx-mn\n",
        "diff = sz/(len(points)-1)\n",
        "ax_points = np.linspace(mn+diff/2, (mx-diff)+diff/2, num)\n",
        "ax.set_xticks(ax_points)\n",
        "ax.set_xticklabels(label_list)\n",
        "ax.set_yticks(ax_points)\n",
        "ax.set_yticklabels(label_list)\n",
        "plt.scatter(x=xs, y=ys, c='teal', alpha=0.7, marker='o', s=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDC9JAL9vawi"
      },
      "outputs": [],
      "source": [
        "#@title Save Sketch\n",
        "sketch_name = 'sketch_1'#@param{type:'string'}\n",
        "if os.path.exists(f'{sketches_path}/{model_name}_{sketch_name}.mov'):\n",
        "  print(f'\\n{model_name}_{sketch_name}.mov already exists, please rename!!\\n\\n')\n",
        "  raise Exception()\n",
        "iter(l)\n",
        "\n",
        "for tick in range(frames):\n",
        "  x, y = next(l)\n",
        "  print(f'Generating x:{x}, y:{y}')\n",
        "  if tensor_pkg_version == 'tensorflow':\n",
        "    if inject_image:\n",
        "      img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs, noise=noise, noise_weight=image_weight)\n",
        "    else:\n",
        "      img = decode_pic(decoder, x, y, num_w, Gs, Gs_kwargs)\n",
        "  elif tensor_pkg_version == 'pytorch':\n",
        "    vae.eval()\n",
        "    zij = vae.decode(torch.tensor(np.array([[x,y]]).astype('float32')).to(device))\n",
        "    zij = np.repeat(zij.detach().cpu().numpy(), num_w, axis=0).reshape(1, num_w, -1)\n",
        "    if inject_image:\n",
        "      zij += image_weight*noise\n",
        "    img = G.synthesis(ws = torch.tensor(zij).to(device), noise_mode='const')[0]\n",
        "    img = (img.permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).detach().cpu().numpy()\n",
        "  Image.fromarray(img.astype('uint8'), 'RGB').save(f'{reel_path}/reel_{tick:09d}.png')\n",
        "\n",
        "print(f'Making Sketch and Saving to {sketches_path}')\n",
        "os.system(f\"ffmpeg -framerate {fps} -pattern_type glob -i '{reel_path}/*.png' -f mov -pix_fmt yuv420p {sketches_path}/{model_name}_{sketch_name}.mov\")\n",
        "os.system(f'rm {reel_path}/*.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8c1NNwHo-KY4"
      ],
      "machine_shape": "hm",
      "name": "Painting with StyleGAN",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
